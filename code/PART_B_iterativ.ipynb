{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, History\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Bidirectional, TimeDistributed\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44267, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO5_price</th>\n",
       "      <th>NO5_load_actual</th>\n",
       "      <th>NO5_load_delta</th>\n",
       "      <th>NO5_generation_actual</th>\n",
       "      <th>NO5_generation_delta</th>\n",
       "      <th>dato_id</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44264</th>\n",
       "      <td>35.69</td>\n",
       "      <td>2459</td>\n",
       "      <td>-241</td>\n",
       "      <td>8286</td>\n",
       "      <td>-4139</td>\n",
       "      <td>2021-01-20 20:00:00+01:00</td>\n",
       "      <td>2021-01-20-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44265</th>\n",
       "      <td>32.19</td>\n",
       "      <td>2385</td>\n",
       "      <td>-308</td>\n",
       "      <td>6884</td>\n",
       "      <td>-3499</td>\n",
       "      <td>2021-01-20 21:00:00+01:00</td>\n",
       "      <td>2021-01-20-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44266</th>\n",
       "      <td>29.49</td>\n",
       "      <td>2326</td>\n",
       "      <td>-360</td>\n",
       "      <td>5496</td>\n",
       "      <td>-2752</td>\n",
       "      <td>2021-01-20 22:00:00+01:00</td>\n",
       "      <td>2021-01-20-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NO5_price  NO5_load_actual  NO5_load_delta  NO5_generation_actual  \\\n",
       "44264      35.69             2459            -241                   8286   \n",
       "44265      32.19             2385            -308                   6884   \n",
       "44266      29.49             2326            -360                   5496   \n",
       "\n",
       "       NO5_generation_delta                    dato_id      date_time  \n",
       "44264                 -4139  2021-01-20 20:00:00+01:00  2021-01-20-20  \n",
       "44265                 -3499  2021-01-20 21:00:00+01:00  2021-01-20-21  \n",
       "44266                 -2752  2021-01-20 22:00:00+01:00  2021-01-20-22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** (44267, 1)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "dataset_df = pd.read_csv(f\"{INPUT_DATA}MAIN_DATASET.csv\")[[\"NO5_price\", \"NO5_load_actual\",\"NO5_load_delta\", \"NO5_generation_actual\", \"NO5_generation_delta\", \"dato_id\",\"date_time\"]][:-7381]; print(dataset_df.shape)\n",
    "\n",
    "display(dataset_df.tail(3))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "print(\"***\",dataset_df[\"NO5_price\"].values.reshape(-1,1).shape)\n",
    "\n",
    "dataset_df[\"NO5_price\"] = scaler.fit_transform(dataset_df[\"NO5_price\"].values.reshape(-1,1))\n",
    "dataset_df[\"NO5_load_actual\"] = scaler.fit_transform(dataset_df[\"NO5_load_actual\"].values.reshape(-1,1))\n",
    "dataset_df[\"NO5_load_delta\"] = scaler.fit_transform(dataset_df[\"NO5_load_delta\"].values.reshape(-1,1))\n",
    "dataset_df[\"NO5_generation_actual\"] = scaler.fit_transform(dataset_df[\"NO5_generation_actual\"].values.reshape(-1,1))\n",
    "dataset_df[\"NO5_generation_delta\"] = scaler.fit_transform(dataset_df[\"NO5_generation_delta\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeqToVec\n",
    "Returning only the last output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1845\n",
      "(1842,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp/ipykernel_40532/3588044864.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dataset = np.array(dataset)\n"
     ]
    }
   ],
   "source": [
    "days = seperate_column_to_days(dataset_df['NO5_price'])\n",
    "\n",
    "lookbehind = 3\n",
    "horizon = 24\n",
    "no_hours = lookbehind*24 + horizon# lookbehind + one horizon\n",
    "hour_in_days = int(no_hours / 24)\n",
    "\n",
    "dataset = []\n",
    "for i in range(len(days)-hour_in_days+1):\n",
    "    dataset.append(np.concatenate((days[i:i+hour_in_days])))\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40532/630798746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "dataset = dataset[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "n,m,k = dataset.shape\n",
    "print(n,m,k)\n",
    "\n",
    "n_steps = m - 24\n",
    "horizon = 24\n",
    "\n",
    "train = int(0.7*n)\n",
    "valid = int(0.9*n)\n",
    "#train = int(0.9*n)\n",
    "\n",
    "# (7000 time series, 50 steps each)\n",
    "X_train, y_train = dataset[:train, :n_steps], dataset[:train, -horizon:, 0]\n",
    "X_valid, y_valid = dataset[train:valid, :n_steps], dataset[train:valid, -horizon: 0]\n",
    "X_test, y_test = dataset[valid:, :n_steps], dataset[valid:, -horizon:, 0]\n",
    "#X_test, y_test = dataset[train:, :n_steps], dataset[train:, -horizon:, 0]\n",
    "\n",
    "\n",
    "# Seq - to - seq does not give satisfying results\n",
    "\n",
    "Y = np.empty((n, n_steps, horizon))\n",
    "for step_ahead in range(1, horizon + 1):\n",
    "    Y[:,:, step_ahead - 1] = dataset[:,step_ahead:step_ahead + n_steps, 0]\n",
    "\n",
    "Y_train = Y[:train]\n",
    "Y_valid = Y[train:valid]\n",
    "Y_test = Y[valid:]\n",
    "\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{X_valid.shape=}\")\n",
    "print(f\"{X_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eta = 0.002\n",
    "batch_size = 200\n",
    "epochs =20\n",
    "model = keras.models.Sequential([\n",
    "    LSTM(25, return_sequences=True, input_shape=[None, 72]),\n",
    "    LSTM(25, return_sequences=True),\n",
    "    TimeDistributed(keras.layers.Dense(1))\n",
    "])\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.000001)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=eta)\n",
    "model.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "callbacks = [learning_rate_reduction]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "history = model.fit(np.swapaxes(X_train, 1, 2), y_train, \n",
    "                    epochs=20, shuffle=False, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(np.swapaxes(X_valid, 1, 2), y_valid),\n",
    "                    callbacks= callbacks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(np.swapaxes(X_valid, 1,2), y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(np.swapaxes(X_valid, 1,2))\n",
    "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtbelVxDOq63"
   },
   "source": [
    "## Forecasting Several Steps Ahead\n",
    "Iterative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series slicing parameters\n",
    "nbf_features = input_window = n_steps = 72 \n",
    "nbf_outputs = output_window = horizon = 24\n",
    "\n",
    "# Creating time series traing and evaluation (test) data besed on sliding window approach\n",
    "time_series, time_series_df = create_time_series_data(dataset_df,                                                                             \n",
    "                                                      nbf_features, \n",
    "                                                      nbf_outputs, \n",
    "                                                      column_key=\"NO5_price\", \n",
    "                                                      column_suffix=\"DA\",\n",
    "                                                      include_target_outputs=True)\n",
    "#time_series_df.to_csv(\"TIMESERIES_VALIDATION.csv\")\n",
    "\n",
    "\n",
    "# Displaying time series data\n",
    "time_series_df.index.name = \"t\"\n",
    "\n",
    "print(time_series.shape)\n",
    "print(time_series_df.shape)\n",
    "# print(time_series_df.columns)\n",
    "\n",
    "X_train, y_train = time_series[ :34272, :input_window], time_series[ :34272, input_window:]\n",
    "X_valid, y_valid = time_series[34272:34272+6581, :input_window], time_series[34272:34272+6581, input_window:]\n",
    "X_test, y_test = time_series[34272+6581:, :input_window], time_series[34272+6581:, input_window:]\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_valid = np.expand_dims(X_valid, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43) # not 42, as it would give the first series in the train set\n",
    "\n",
    "X_new = X_test[-1:, :]\n",
    "Y_new = np.expand_dims(y_test[-1:, :], 2)\n",
    "\n",
    "display(dataset_df[[\"NO5_price\", \"date_time\"]].tail(1))\n",
    "\n",
    "print(f\"{X_new.shape=}\")\n",
    "print(f\"{Y_new.shape=}\")\n",
    "\n",
    "# X = X_new\n",
    "X = np.swapaxes(X_new, 1,2)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "for step_ahead in range(horizon):\n",
    "    # y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    y_pred_one = model.predict(X[:, :, step_ahead:])\n",
    "\n",
    "    print(f\"{y_pred_one.shape=}\")\n",
    "    #y_pred_one = y_pred_one[:, np.newaxis, :]\n",
    "\n",
    "    # X = np.concatenate([X, y_pred_one], axis=1)\n",
    "    print(f\"{y_pred_one.shape=}\")\n",
    "\n",
    "    X = np.concatenate([X, y_pred_one], axis=2)\n",
    "    print(f\"{X.shape=}\")\n",
    "\n",
    "print(f\"{X.shape=}\")\n",
    "Y_pred = X[:, n_steps:]\n",
    "Y_pred = X[:,:, n_steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.swapaxes(Y_pred, 1,2)\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_forecasts(X_new, Y_new, Y_pred)\n",
    "save_fig(\"forecast_ahead_plot\")\n",
    "plt.ylim(0.10,0.70)\n",
    "plt.show()\n",
    "\n",
    "mse = keras.metrics.mean_squared_error(Y_new, Y_pred)\n",
    "print(\"MSE:\",np.mean(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series slicing parameters\n",
    "nbf_features = input_window = n_steps = 72 \n",
    "nbf_outputs = output_window = 24\n",
    "\n",
    "\n",
    "# Creating time series traing and evaluation (test) data besed on sliding window approach\n",
    "time_series, time_series_df = create_time_series_data(dataset_df,                                                                             \n",
    "                                                      nbf_features, \n",
    "                                                      nbf_outputs, \n",
    "                                                      column_key=\"NO5_price\", \n",
    "                                                      column_suffix=\"DA\",\n",
    "                                                      include_target_outputs=True)\n",
    "#time_series_df.to_csv(\"TIMESERIES_VALIDATION.csv\")\n",
    "\n",
    "\n",
    "# Displaying time series data\n",
    "time_series_df.index.name = \"t\"\n",
    "\n",
    "print(time_series.shape)\n",
    "print(time_series_df.shape)\n",
    "# print(time_series_df.columns)\n",
    "\n",
    "X_train, y_train = time_series[ :34272, :input_window], time_series[ :34272, input_window:]\n",
    "X_valid, y_valid = time_series[34272:34272+6581, :input_window], time_series[34272:34272+6581, input_window:]\n",
    "X_test, y_test = time_series[34272+6581:, :input_window], time_series[34272+6581:, input_window:]\n",
    "\n",
    "# X_train = np.expand_dims(X_train, axis=2)\n",
    "# X_valid = np.expand_dims(X_valid, axis=2)\n",
    "# X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLR_model = LinearRegression()\n",
    "MLR_model.fit(X_train, y_train[:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = X_test[-1:, :]\n",
    "Y_new = y_test[-1:, :]\n",
    "\n",
    "X = X_new\n",
    "\n",
    "\n",
    "for step_ahead in range(24):\n",
    "    # y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    y_pred_one = MLR_model.predict(X[:, step_ahead:])\n",
    "\n",
    "    print(y_pred_one)\n",
    "\n",
    "    print(f\"{y_pred_one.shape=}\")\n",
    "    #y_pred_one = y_pred_one[:, np.newaxis, :]\n",
    "\n",
    "    # X = np.concatenate([X, y_pred_one], axis=1)\n",
    "    print(f\"{y_pred_one.shape=}\")\n",
    "\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "    print(f\"{X.shape=}\")\n",
    "\n",
    "print(f\"{X.shape=}\")\n",
    "Y_pred = X[:, n_steps:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.expand_dims(X_new,2)\n",
    "Y_new = np.expand_dims(Y_new,2)\n",
    "Y_pred = np.expand_dims(Y_pred,2)\n",
    "\n",
    "plot_multiple_forecasts(X_new, Y_new, Y_pred)\n",
    "save_fig(\"forecast_ahead_plot\")\n",
    "plt.ylim(0.10,0.70)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mse = keras.metrics.mean_squared_error(Y_new, Y_pred)\n",
    "print(\"MSE:\",np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_project2 import *\n",
    "eta = 0.05\n",
    "lmb = 0\n",
    "hidden_size = 64\n",
    "act_func = \"relu\"\n",
    "act_func = \"leaky_relu\"\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_model, tf_model = NN_simple_architecture(eta, nbf_features,\n",
    "                                                         problem_type=\"regression\",\n",
    "                                                         nbf_outputs=1,\n",
    "                                                         X_test=X_test.copy(), t_test=y_test.copy()[:, :1],\n",
    "                                                         lmb=lmb, \n",
    "                                                         hidden_size=hidden_size, \n",
    "                                                         act_func=act_func)\n",
    "\n",
    "own_model, tf_model = NN_large_architecture(eta, nbf_features,\n",
    "                                                         problem_type=\"regression\",\n",
    "                                                         nbf_outputs=1,\n",
    "                                                         X_test=X_test.copy(), t_test=y_test.copy()[:, :1],\n",
    "                                                         lmb=lmb, \n",
    "                                                         hidden_size=hidden_size, \n",
    "                                                         act_func=act_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[:, :1])\n",
    "\n",
    "train_losses, test_losses = own_model.fit(X_train, y_train[:, :1], \n",
    "                                                            batch_size=batch_size, epochs=epochs,\n",
    "                                                            lr_scheduler=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x_plot = np.arange(train_losses.shape[0])\n",
    "plt.plot(x_plot, train_losses)\n",
    "plt.plot(x_plot, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_hat_test_own = own_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(43) # not 42, as it would give the first series in the train set\n",
    "\n",
    "X_new = X_test[-1:, :]\n",
    "Y_new = y_test[-1:, :]\n",
    "\n",
    "X = X_new\n",
    "\n",
    "\n",
    "for step_ahead in range(24):\n",
    "    # y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    y_pred_one = own_model.predict(X[:, step_ahead:])\n",
    "\n",
    "    print(y_pred_one)\n",
    "\n",
    "    print(f\"{y_pred_one.shape=}\")\n",
    "    #y_pred_one = y_pred_one[:, np.newaxis, :]\n",
    "\n",
    "    # X = np.concatenate([X, y_pred_one], axis=1)\n",
    "    print(f\"{y_pred_one.shape=}\")\n",
    "\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "    print(f\"{X.shape=}\")\n",
    "\n",
    "print(f\"{X.shape=}\")\n",
    "Y_pred = X[:, n_steps:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.expand_dims(X_new,2)\n",
    "Y_new = np.expand_dims(Y_new,2)\n",
    "Y_pred = np.expand_dims(Y_pred,2)\n",
    "print(f\"{Y_pred.shape=}\")\n",
    "print(f\"{Y_new.shape=}\")\n",
    "print(f\"{X_new.shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_forecasts(X_new, Y_new, Y_pred)\n",
    "save_fig(\"forecast_ahead_plot\")\n",
    "plt.ylim(0.10,0.70)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mse = keras.metrics.mean_squared_error(Y_new, Y_pred)\n",
    "print(\"MSE:\",np.mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "664767e83c7b06364e16bf4bb7694b1f8b5947c6e3e0843db3deeed47f8be06a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('fysstk1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

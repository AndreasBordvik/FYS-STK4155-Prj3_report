% Encoding: UTF-8
@Book{Bishop2016,
  author    = {Bishop, Christopher M.},
  publisher = {Springer New York},
  title     = {Pattern Recognition and Machine Learning},
  year      = {2016},
  isbn      = {1493938436},
  month     = aug,
  ean       = {9781493938438},
  pagetotal = {760},
  url       = {https://www.ebook.de/de/product/29900601/christopher_m_bishop_pattern_recognition_and_machine_learning.html},
}

@Book{Hastie2009,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  publisher = {SPRINGER NATURE},
  title     = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
  year      = {2009},
  isbn      = {0387848576},
  month     = feb,
  ean       = {9780387848570},
  pagetotal = {745},
  url       = {https://www.ebook.de/de/product/8023140/trevor_hastie_robert_tibshirani_jerome_friedman_the_elements_of_statistical_learning_data_mining_inference_and_prediction_second_edition.html},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@Book{Davison1997,
  author    = {A. C. Davison and D. V. Hinkley},
  publisher = {Cambridge University Press},
  title     = {Bootstrap Methods and their Application},
  year      = {1997},
  month     = {oct},
  doi       = {10.1017/cbo9780511802843},
}

@Book{Geron2019,
  author    = {Géron, Aurélien},
  publisher = {O'Reilly UK Ltd.},
  title     = {Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow},
  year      = {2019},
  isbn      = {1492032646},
  month     = oct,
  ean       = {9781492032649},
  pagetotal = {819},
  url       = {https://www.ebook.de/de/product/33315532/aurelien_geron_hands_on_machine_learning_with_scikit_learn_keras_and_tensorflow.html},
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@Article{Wolpert1996,
  author    = {David H. Wolpert},
  journal   = {Neural Computation},
  title     = {The Lack of A Priori Distinctions Between Learning Algorithms},
  year      = {1996},
  month     = {oct},
  number    = {7},
  pages     = {1341--1390},
  volume    = {8},
  doi       = {10.1162/neco.1996.8.7.1341},
  publisher = {{MIT} Press - Journals},
}

@Article{Hornik1989,
  author    = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  title     = {Multilayer feedforward networks are universal approximators},
  year      = {1989},
  month     = {jan},
  number    = {5},
  pages     = {359--366},
  volume    = {2},
  doi       = {10.1016/0893-6080(89)90020-8},
  publisher = {Elsevier {BV}},
}

@Article{Cybenko1989,
  author    = {G. Cybenko},
  title     = {Approximation by superpositions of a sigmoidal function},
  year      = {1989},
  month     = {dec},
  number    = {4},
  pages     = {303--314},
  volume    = {2},
  doi       = {10.1007/bf02551274},
  publisher = {Springer Science and Business Media {LLC}},
}

@Book{Nielsen2015,
  author    = {Michael A. Nielsen},
  publisher = {Determination Press},
  title     = {Neural Networks and Deep Learning},
  year      = {2015},
  note      = {Last update: Thu Dec 26 15:26:33 2019},
}

@Article{Paszke2019,
  author        = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  title         = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year          = {2019},
  month         = dec,
  abstract      = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  eprint        = {1912.01703},
  file          = {:http\://arxiv.org/pdf/1912.01703v1:PDF},
  keywords      = {cs.LG, cs.MS, stat.ML},
  primaryclass  = {cs.LG},
}

@Misc{Crameri2021,
  author    = {Crameri, Fabio},
  title     = {Scientific colour maps},
  year      = {2021},
  copyright = {MIT License},
  doi       = {10.5281/ZENODO.5501399},
  keywords  = {scientific colour maps, scientific colormap, batlow, scientific colourmap, colormap, perceptually uniform, colour vision deficiency, color map, colour palettes, colour schemes, color gradient, color palette, color scheme, CVD, visualisation, scientific visualisation, color perception, data representation},
  language  = {en},
  publisher = {Zenodo},
}

@Misc{tensorflow2015-whitepaper,
  author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {https://www.tensorflow.org/},
}

@Article{Rumelhart1986,
  author    = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title     = {Learning representations by back-propagating errors},
  year      = {1986},
  month     = {oct},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  doi       = {10.1038/323533a0},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Glorot2010,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  year      = {2010},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  editor    = {Teh, Yee Whye and Titterington, Mike},
  month     = {13--15 May},
  pages     = {249--256},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {9},
  abstract  = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  pdf       = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url       = {https://proceedings.mlr.press/v9/glorot10a.html},
}

@Article{He2015,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  year          = {2015},
  month         = feb,
  abstract      = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  eprint        = {1502.01852},
  file          = {:http\://arxiv.org/pdf/1502.01852v1:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
}

@Comment{jabref-meta: databaseType:bibtex;}

% Encoding: UTF-8
@book{Bishop2016,
  author    = {Bishop, Christopher M.},
  publisher = {Springer New York},
  title     = {Pattern Recognition and Machine Learning},
  year      = {2016},
  isbn      = {1493938436},
  month     = aug,
  ean       = {9781493938438},
  pagetotal = {760},
  url       = {https://www.ebook.de/de/product/29900601/christopher_m_bishop_pattern_recognition_and_machine_learning.html}
}

@book{Hastie2009,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  publisher = {SPRINGER NATURE},
  title     = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
  year      = {2009},
  isbn      = {0387848576},
  month     = feb,
  ean       = {9780387848570},
  pagetotal = {745},
  url       = {https://www.ebook.de/de/product/8023140/trevor_hastie_robert_tibshirani_jerome_friedman_the_elements_of_statistical_learning_data_mining_inference_and_prediction_second_edition.html}
}


@book{Auffarth2021,
  author    = {Ben Auffarth},
  publisher = {Packt Publishing},
  title     = {Machine Learning for Time-Series with Python},
  year      = {2021},
  isbn      = {978-1-80181-962-6},
  month     = october,
  ean       = {978-1-80181-962-6},
  pagetotal = {352},
  url       = {https://www.bookdepository.com/Machine-Learning-for-Time-Series-with-Python-Ben-Auffarth/9781801819626}
}


@book{Korstanje2021,
  author    = {Joos Korstanje},
  publisher = {Apress},
  title     = {Advanced Forecasting with Python},
  year      = {2021},
  isbn      = {978-1-4842-7150-6},
  pagetotal = {296},
  url       = {https://link.springer.com/book/10.1007/978-1-4842-7150-6}
}

@book{Vishwas2020,
  author    = {B V Vishwas, ASHISH PATEL},
  publisher = {Apress},
  title     = {Hands-on Time Series Analysis with Python},
  year      = {2020},
  isbn      = {978-1-4842-5992-4},
  pagetotal = {407},
  url       = {https://link.springer.com/book/10.1007/978-1-4842-5992-4#about}
}

@book{Nielsen2019,
  author    = {Aileen Nielsen},
  publisher = {O'Reilly Media, Inc.},
  title     = {Practical Time Series Analysis},
  year      = {2019},
  isbn      = {9781492041658},
  pagetotal = {480},
  url       = {https://www.oreilly.com/library/view/practical-time-series/9781492041641/}
}

@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}

@book{Davison1997,
  author    = {A. C. Davison and D. V. Hinkley},
  publisher = {Cambridge University Press},
  title     = {Bootstrap Methods and their Application},
  year      = {1997},
  month     = {oct},
  doi       = {10.1017/cbo9780511802843}
}

@book{Geron2019,
  author    = {Géron, Aurélien},
  publisher = {O'Reilly UK Ltd.},
  title     = {Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow},
  year      = {2019},
  isbn      = {1492032646},
  month     = oct,
  ean       = {9781492032649},
  pagetotal = {819},
  url       = {https://www.ebook.de/de/product/33315532/aurelien_geron_hands_on_machine_learning_with_scikit_learn_keras_and_tensorflow.html}
}

@book{Goodfellow2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}

@article{Wolpert1996,
  author    = {David H. Wolpert},
  journal   = {Neural Computation},
  title     = {The Lack of A Priori Distinctions Between Learning Algorithms},
  year      = {1996},
  month     = {oct},
  number    = {7},
  pages     = {1341--1390},
  volume    = {8},
  doi       = {10.1162/neco.1996.8.7.1341},
  publisher = {{MIT} Press - Journals}
}

@article{Hornik1989,
  author    = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  title     = {Multilayer feedforward networks are universal approximators},
  year      = {1989},
  month     = {jan},
  number    = {5},
  pages     = {359--366},
  volume    = {2},
  doi       = {10.1016/0893-6080(89)90020-8},
  publisher = {Elsevier {BV}}
}

@article{Cybenko1989,
  author    = {G. Cybenko},
  title     = {Approximation by superpositions of a sigmoidal function},
  year      = {1989},
  month     = {dec},
  number    = {4},
  pages     = {303--314},
  volume    = {2},
  doi       = {10.1007/bf02551274},
  publisher = {Springer Science and Business Media {LLC}}
}

@book{Nielsen2015,
  author    = {Michael A. Nielsen},
  publisher = {Determination Press},
  title     = {Neural Networks and Deep Learning},
  year      = {2015},
  note      = {Last update: Thu Dec 26 15:26:33 2019}
}

@article{Paszke2019,
  author        = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  title         = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year          = {2019},
  month         = dec,
  abstract      = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  eprint        = {1912.01703},
  file          = {:http\://arxiv.org/pdf/1912.01703v1:PDF},
  keywords      = {cs.LG, cs.MS, stat.ML},
  primaryclass  = {cs.LG}
}

@misc{Crameri2021,
  author    = {Crameri, Fabio},
  title     = {Scientific colour maps},
  year      = {2021},
  copyright = {MIT License},
  doi       = {10.5281/ZENODO.5501399},
  keywords  = {scientific colour maps, scientific colormap, batlow, scientific colourmap, colormap, perceptually uniform, colour vision deficiency, color map, colour palettes, colour schemes, color gradient, color palette, color scheme, CVD, visualisation, scientific visualisation, color perception, data representation},
  language  = {en},
  publisher = {Zenodo}
}

@misc{tensorflow2015-whitepaper,
  author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {https://www.tensorflow.org/}
}

@inproceedings{seabold2010statsmodels,
  title     = {statsmodels: Econometric and statistical modeling with python},
  author    = {Seabold, Skipper and Perktold, Josef},
  booktitle = {9th Python in Science Conference},
  year      = {2010}
}


@article{Rumelhart1986,
  author    = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title     = {Learning representations by back-propagating errors},
  year      = {1986},
  month     = {oct},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  doi       = {10.1038/323533a0},
  publisher = {Springer Science and Business Media {LLC}}
}

@inproceedings{Glorot2010,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  year      = {2010},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  editor    = {Teh, Yee Whye and Titterington, Mike},
  month     = {13--15 May},
  pages     = {249--256},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {9},
  abstract  = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  pdf       = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url       = {https://proceedings.mlr.press/v9/glorot10a.html}
}

@article{He2015,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  year          = {2015},
  month         = feb,
  abstract      = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  eprint        = {1502.01852},
  file          = {:http\://arxiv.org/pdf/1502.01852v1:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV}
}

@article{Hochreiter1997,
  author  = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal = {Neural Computation},
  title   = {Long Short-Term Memory},
  year    = {1997},
  volume  = {9},
  number  = {8},
  pages   = {1735-1780},
  doi     = {10.1162/neco.1997.9.8.1735}
}

@article{Greff2015,
  author  = {Greff, Klaus and Srivastava, Rupesh K. and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  title   = {LSTM: A Search Space Odyssey},
  year    = {2017},
  volume  = {28},
  number  = {10},
  pages   = {2222-2232},
  doi     = {10.1109/TNNLS.2016.2582924}
}

@article{Murugan2018,
  author     = {Pushparaja Murugan},
  title      = {Learning The Sequential Temporal Information with Recurrent Neural
                Networks},
  journal    = {CoRR},
  volume     = {abs/1807.02857},
  year       = {2018},
  url        = {http://arxiv.org/abs/1807.02857},
  eprinttype = {arXiv},
  eprint     = {1807.02857},
  timestamp  = {Mon, 13 Aug 2018 16:46:34 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1807-02857.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@Comment{jabref-meta: databaseType:bibtex;}


@article{andrade_probabilistic_2017,
  title      = {Probabilistic {Price} {Forecasting} for {Day}-{Ahead} and {Intraday} {Markets}: {Beyond} the {Statistical} {Model}},
  volume     = {9},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  shorttitle = {Probabilistic {Price} {Forecasting} for {Day}-{Ahead} and {Intraday} {Markets}},
  url        = {https://www.mdpi.com/2071-1050/9/11/1990},
  doi        = {10.3390/su9111990},
  abstract   = {Forecasting the hourly spot price of day-ahead and intraday markets is particularly challenging in electric power systems characterized by high installed capacity of renewable energy technologies. In particular, periods with low and high price levels are difficult to predict due to a limited number of representative cases in the historical dataset, which leads to forecast bias problems and wide forecast intervals. Moreover, these markets also require the inclusion of multiple explanatory variables, which increases the complexity of the model without guaranteeing a forecasting skill improvement. This paper explores information from daily futures contract trading and forecast of the daily average spot price to correct point and probabilistic forecasting bias. It also shows that an adequate choice of explanatory variables and use of simple models like linear quantile regression can lead to highly accurate spot price point and probabilistic forecasts. In terms of point forecast, the mean absolute error was 3.03 €/MWh for day-ahead market and a maximum value of 2.53 €/MWh was obtained for intraday session 6. The probabilistic forecast results show sharp forecast intervals and deviations from perfect calibration below 7\% for all market sessions.},
  language   = {en},
  number     = {11},
  urldate    = {2020-11-08},
  journal    = {Sustainability},
  author     = {Andrade, José R. and Filipe, Jorge and Reis, Marisa and Bessa, Ricardo J.},
  month      = nov,
  year       = {2017},
  keywords   = {electricity market, feature engineering, intraday, price forecasting, statistical learning, uncertainty},
  pages      = {1990},
  annote     = {Number: 11 Publisher: Multidisciplinary Digital Publishing Institute}
}

@article{li_day-ahead_2021,
  title    = {Day-ahead electricity price prediction applying hybrid models of {LSTM}-based deep learning methods and feature selection algorithms under consideration of market coupling},
  volume   = {237},
  issn     = {0360-5442},
  url      = {https://www.sciencedirect.com/science/article/pii/S0360544221017916},
  doi      = {10.1016/j.energy.2021.121543},
  abstract = {The availability of accurate day-ahead electricity price forecasts is pivotal for electricity market participants. In the context of trade liberalisation and market harmonisation in the European markets, accurate price forecasting becomes difficult for electricity market participants to obtain because electricity forecasting requires the consideration of features from ever-growing coupling markets. This study provides a method of exploring the influence of market coupling on electricity price prediction. We apply state-of-the-art long short-term memory (LSTM) deep neural networks combined with feature selection algorithms for electricity price prediction under the consideration of market coupling. LSTM models have a good performance in handling nonlinear and complex problems and processing time series data. In our empirical study of the Nordic market, the proposed models obtain considerably accurate results. The results show that feature selection is essential to achieving accurate prediction, and features from integrated markets have an impact on prediction. The feature importance analysis implies that the German market has a salient role in the price generation of Nord Pool.},
  language = {en},
  urldate  = {2021-12-14},
  journal  = {Energy},
  author   = {Li, Wei and Becker, Denis Mike},
  month    = dec,
  year     = {2021},
  keywords = {Deep learning, Electricity market coupling, Electricity price forecasting (EPF), Feature selection, Long short-term memory (LSTM), The Nord Pool system price, Price},
  pages    = {121543},
  file     = {ScienceDirect Full Text PDF:C\:\\Users\\andre\\Zotero\\storage\\J7XSH4GY\\Li and Becker - 2021 - Day-ahead electricity price prediction applying hy.pdf:application/pdf}
}

@article{nowotarski_recent_2018,
  title      = {Recent advances in electricity price forecasting: {A} review of probabilistic forecasting},
  volume     = {81},
  issn       = {1364-0321},
  shorttitle = {Recent advances in electricity price forecasting},
  url        = {https://www.sciencedirect.com/science/article/pii/S1364032117308808},
  doi        = {10.1016/j.rser.2017.05.234},
  abstract   = {Since the inception of competitive power markets two decades ago, electricity price forecasting (EPF) has gradually become a fundamental process for energy companies’ decision making mechanisms. Over the years, the bulk of research has concerned point predictions. However, the recent introduction of smart grids and renewable integration requirements has had the effect of increasing the uncertainty of future supply, demand and prices. Academics and practitioners alike have come to understand that probabilistic electricity price (and load) forecasting is now more important for energy systems planning and operations than ever before. With this paper we offer a tutorial review of probabilistic EPF and present much needed guidelines for the rigorous use of methods, measures and tests, in line with the paradigm of ‘maximizing sharpness subject to reliability’. The paper can be treated as an update and a further extension of the otherwise comprehensive EPF review of Weron [1] or as a standalone treatment of a fascinating and underdeveloped topic, that has a much broader reach than EPF itself.},
  language   = {en},
  urldate    = {2021-12-14},
  journal    = {Renewable and Sustainable Energy Reviews},
  author     = {Nowotarski, Jakub and Weron, Rafał},
  month      = jan,
  year       = {2018},
  keywords   = {Autoregression, Day-ahead market, Electricity price forecasting, Neural network, Probabilistic forecast, Reliability, Sharpness, Price},
  pages      = {1548--1568},
  file       = {ScienceDirect Snapshot:C\:\\Users\\andre\\Zotero\\storage\\7FVKHUV6\\S1364032117308808.html:text/html}
}


@misc{nordpool,
  title    = {Markets divided into bidding areas.},
  url      = {https://www.nordpoolgroup.com/the-power-market/Bidding-areas/},
  abstract = {The day-ahead market is an auction where power is traded for delivery each hour the next day. The Nord Pool markets are divided into several bidding areas. The available transmission capacity may vary and congest the flow of power between the bidding areas, and thereby different area prices are established.},
  urldate  = {2021-12-14},
  file     = {Snapshot:C\:\\Users\\andre\\Zotero\\storage\\QHY7JPKK\\Bidding-areas.html:text/html}
}


@article{nogales_forecasting_2002,
  title    = {Forecasting next-day electricity prices by time series models},
  volume   = {17},
  issn     = {1558-0679},
  doi      = {10.1109/TPWRS.2002.1007902},
  abstract = {In the framework of competitive electricity markets, power producers and consumers need accurate price forecasting tools. Price forecasts embody crucial information for producers and consumers when planning bidding strategies in order to maximize their benefits and utilities, respectively. This paper provides two highly accurate yet efficient price forecasting tools based on time series analysis: dynamic regression and transfer function models. These techniques are explained and checked against each other. Results and discussions from real-world case studies based on the electricity markets of mainland Spain and California are presented.},
  number   = {2},
  journal  = {IEEE Transactions on Power Systems},
  author   = {Nogales, F.J. and Contreras, J. and Conejo, A.J. and Espinola, R.},
  month    = may,
  year     = {2002},
  note     = {Conference Name: IEEE Transactions on Power Systems},
  keywords = {Aggregates, Contracts, Delay, Economic forecasting, Electricity supply industry, Power generation, Predictive models, Strategic planning, Supply and demand, Time series analysis},
  pages    = {342--348},
  file     = {IEEE Xplore Abstract Record:C\:\\Users\\andre\\Zotero\\storage\\PYXIQ7TY\\1007902.html:text/html}
}

@article{bunn_forecasting_2000,
  title    = {Forecasting loads and prices in competitive power markets},
  volume   = {88},
  issn     = {1558-2256},
  doi      = {10.1109/5.823996},
  abstract = {This paper provides a review of some of the main methodological issues and techniques which have become innovative in addressing the problem of forecasting daily loads and prices in the new competitive power markets. Particular emphasis is placed upon computationally intensive methods, including variable segmentation, multiple modeling, combinations, and neural networks for forecasting the demand side, and strategic simulation using artificial agents for the supply side.},
  number   = {2},
  journal  = {Proceedings of the IEEE},
  author   = {Bunn, D.W.},
  month    = feb,
  year     = {2000},
  note     = {Conference Name: Proceedings of the IEEE},
  keywords = {Computational modeling, Costs, Demand forecasting, Economic forecasting, Load forecasting, Monopoly, Power markets, Predictive models, Uncertainty, Weather forecasting},
  pages    = {163--169},
  file     = {IEEE Xplore Abstract Record:C\:\\Users\\andre\\Zotero\\storage\\23TMFBDY\\823996.html:text/html}
}

@misc{noauthor_magasinstatistikk_nodate,
  title    = {Magasinstatistikk - {NVE}},
  url      = {https://www.nve.no/energi/analyser-og-statistikk/magasinstatistikk/},
  abstract = {Norge har over 1600 vannkraftverk som står for 96 \% av kraftproduksjonen i landet. Hver uke samler Norges vassdrags- og energidirektorat inn vannstandsmålinger fra de 489 viktigste vannmagasinene for å holde oversikt over kraftsituasjonen i landet.},
  language = {no},
  urldate  = {2021-12-14},
  file     = {Snapshot:C\:\\Users\\andre\\Zotero\\storage\\8UWSU2NM\\magasinstatistikk.html:text/html}
}

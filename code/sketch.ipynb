{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from common import standard_scaling\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_length(t, t0, t1):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "def new_sgd(X_train, t_train, theta, n_epoch, batch_size, eta, lr_scheduler=False, ridge=False, lmb=0):\n",
    "    n_batches = int(X_train.shape[0] // batch_size)\n",
    "    Xt = np.concatenate((X_train, t_train), axis=1)\n",
    "    print(f\"Number of minibatches: {n_batches}\")\n",
    "    \n",
    "    if lr_scheduler:\n",
    "        t0 = 1.0; t1 = 10\n",
    "        eta = t0/t1\n",
    "        print(f\"Using learning rate scheduler with initial learning rate: {eta}\")\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch), f\"Training {n_epoch} epochs\"):      \n",
    "        batches = np.take(Xt, np.random.permutation(Xt.shape[0]), axis=0)\n",
    "        batches = np.array_split(batches, n_batches, axis=0)\n",
    "        \n",
    "        for batch in batches:\n",
    "            xi = batch[:, :-1]\n",
    "            yi = batch[:, -1].reshape(-1,1)\n",
    "            \n",
    "            gradients = 2.0* xi.T @ ((xi @ theta)-yi)\n",
    "            if ridge:\n",
    "                # TODO: the coff regularization is not implemented correct. \n",
    "                gradients +=  lmb*np.eye(theta.shape[0])\n",
    "            \n",
    "            theta = theta - eta*gradients\n",
    "\n",
    "            if lr_scheduler:\n",
    "                t = epoch*n_batches+epoch\n",
    "                eta = step_length(t, t0, t1)\n",
    "            \n",
    "\n",
    "    print(f\"theta from new SGD: {theta.ravel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of minibatches: 160\n",
      "Using learning rate scheduler with initial learning rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1000 epochs: 100%|██████████| 1000/1000 [00:03<00:00, 250.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta from new SGD: [0.8677322]\n",
      "sgdreg from scikit: [0.8734911]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(4155)\n",
    "\n",
    "n = 1000\n",
    "x = 2*np.random.rand(n,1)\n",
    "t = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "X = X[:,1:]\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, shuffle=True)\n",
    "X_train, X_test = standard_scaling(X_train, X_test)\n",
    "t_train, t_test = standard_scaling(t_train, t_test)\n",
    "\n",
    "_,features_X = X_train.shape \n",
    "theta = np.random.randn(features_X,1)\n",
    "eta = 0.1\n",
    "\n",
    "n_epochs = 100000\n",
    "batch_size = 5  #size of each minibatch\n",
    "lr_scheduler = True\n",
    "new_sgd(X_train, t_train, theta, n_epochs, batch_size, eta, lr_scheduler=lr_scheduler)\n",
    "\n",
    "sgdreg = SGDRegressor(max_iter = n_epochs, penalty=None, eta0=eta)\n",
    "sgdreg.fit(X_train,t_train.ravel())\n",
    "print(f\"sgdreg from scikit: {sgdreg.coef_}\")\n",
    "# print(f\"sgdreg from scikit: {sgdreg.intercept_}, {sgdreg.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sketch of SGd based on code imported from the lecture week40 thursday\n",
    "\n",
    "# Importing various packages\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "n = 1000\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "theta_linreg = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "sgdreg = SGDRegressor(max_iter = n_epochs, penalty=None, eta0=0.1)\n",
    "sgdreg.fit(x,y.ravel())\n",
    "print(\"sgdreg from scikit\")\n",
    "print(sgdreg.intercept_, sgdreg.coef_)\n",
    "\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 0.1\n",
    "Niterations = 1000\n",
    "\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/n*X.T @ ((X @ theta)-y)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "\n",
    "\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = M, n_epochs\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2.0* xi.T @ ((xi @ theta)-yi)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)\n",
    "\n",
    "ypredict3 = Xnew.dot(theta)\n",
    "\n",
    "plt.plot(x, y ,'ko', alpha=0.2)\n",
    "plt.plot(xnew, ypredict, \"m-\", label=\"Theta from GD\")\n",
    "plt.plot(xnew, ypredict2, \"b-\", label=\"Theta from Linreg\")\n",
    "plt.plot(xnew, ypredict3, \"c-\", label=\"Theta from SGD\")\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_sgd(X_train, t_train, theta, n_epoch, M):\n",
    "    m = int(X_train.shape[0]/M) #number of minibatches\n",
    "    print(f\"Number of minibatches: {m}\")\n",
    "    for epoch in tqdm(range(n_epoch), f\"Looping throug {n_epoch} epochs\"):\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        drawn = []\n",
    "        for i in range(m):\n",
    "            # Draw datapoints to the batch\n",
    "            idx = np.random.choice(np.delete(indices, drawn), M, replace=False)\n",
    "            drawn.append(idx)\n",
    "            print(f\"Minibatch: {idx}\")\n",
    "            \n",
    "            # Train the model\n",
    "            xi = X_train[idx]\n",
    "            yi = t_train[idx]\n",
    "            gradients = 2.0* xi.T @ ((xi @ theta)-yi)\n",
    "            #eta = learning_schedule(epoch*m+i)\n",
    "            eta = 0.1\n",
    "            theta = theta - eta*gradients\n",
    "        \n",
    "        remain = np.delete(indices, drawn)\n",
    "        if np.any(remain):\n",
    "            print(f\"Training on remaing datapoints: {remain}\")\n",
    "            xi = X_train[remain]\n",
    "            yi = t_train[remain]\n",
    "            gradients = 2.0* xi.T @ ((xi @ theta)-yi)\n",
    "            #eta = learning_schedule(epoch*m+i)\n",
    "            eta = 0.1\n",
    "            theta = theta - eta*gradients\n",
    "  \n",
    "    print(f\"theta from own SGD: {theta.ravel()}\")\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "def _train_sgd(X_train, t_train, batch, theta, epoch, i, m):\n",
    "    xi = X_train[batch]\n",
    "    yi = t_train[batch]\n",
    "    gradients = 2.0* xi.T @ ((xi @ theta)-yi)\n",
    "    eta = learning_schedule(epoch*m+i)\n",
    "    theta = theta - eta*gradients\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
